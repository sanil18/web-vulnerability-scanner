import requests
from bs4 import BeautifulSoup
import re
from packaging import version
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import tldextract
import whois
import datetime
import logging
import concurrent.futures

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebVulnerabilityScanner:
    def __init__(self, target_url):
        self.target_url = target_url
        self.session = requests.Session()
        self.vulnerabilities = []
        self.logger = logger

    def scan_sql_injection(self):
        self.logger.info("Scanning for SQL Injection vulnerabilities...")
        try:
            response = self.session.get(self.target_url)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                sql_patterns = ["sql", "select", "from", "where", "union", "1=1"]

                for pattern in sql_patterns:
                    if re.search(fr'\b{re.escape(pattern)}\b', soup.text, re.IGNORECASE):
                        self.logger.warning(f"Potential SQL Injection found: {pattern}")
                        self.vulnerabilities.append({'type': 'SQL Injection', 'details': f"Potential SQL Injection found: {pattern}"})

                self.logger.info("SQL Injection scan completed.")
            else:
                self.logger.error(f"Failed to fetch the URL. Status code: {response.status_code}")
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error scanning for SQL Injection vulnerabilities: {e}")

    def scan_phishing(self):
        self.logger.info("Scanning for Phishing attempts using Machine Learning...")
        try:
            phish_data = requests.get("https://www.phishtank.com/developer_info.php").json()
            urls = [entry['url'] for entry in phish_data]
            labels = [entry['valid'] for entry in phish_data]

            vectorizer = TfidfVectorizer()
            X = vectorizer.fit_transform(urls)
            X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)

            classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, alpha=0.0001,
                                       solver='sgd', verbose=10,  random_state=42, tol=0.0001)
            classifier.fit(X_train, y_train)

            prediction = classifier.predict(vectorizer.transform([self.target_url]))
            if prediction[0] == 1:
                self.logger.warning("Potential Phishing attempt detected.")
                self.vulnerabilities.append({'type': 'Phishing', 'details': 'Potential Phishing attempt detected.'})
            else:
                self.logger.info("No Phishing attempt detected.")
        except Exception as e:
            self.logger.error(f"Error scanning for Phishing attempts: {e}")

    def scan_xss(self):
        self.logger.info("Scanning for Cross-Site Scripting (XSS) vulnerabilities...")
        try:
            response = self.session.get(self.target_url)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                scripts = soup.find_all('script')

                for script in scripts:
                    if re.search(r'(<\s*script[^>]*>.*<\s*/\s*script\s*>)', str(script), re.IGNORECASE):
                        self.logger.warning("Potential XSS vulnerability found.")
                        self.vulnerabilities.append({'type': 'XSS', 'details': 'Potential XSS vulnerability found.'})
                        break

                self.logger.info("XSS scan completed.")
            else:
                self.logger.error(f"Failed to fetch the URL. Status code: {response.status_code}")
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error scanning for XSS vulnerabilities: {e}")

    def scan_outdated_software(self):
        self.logger.info("Scanning for outdated software versions...")
        try:
            headers = {'User-Agent': 'WebVulnerabilityScanner'}
            response = requests.get(self.target_url, headers=headers)

            server_header = response.headers.get('Server')
            if server_header:
                software_version = extract_version_from_header(server_header)
                if is_outdated(software_version, "1.0.0"):
                    self.logger.warning(f"Outdated software version detected: {software_version}")
                    self.vulnerabilities.append({'type': 'Outdated Software', 'details': f"Outdated software version detected: {software_version}"})
                else:
                    self.logger.info("Software is up to date.")
            else:
                self.logger.warning("Version information not available in HTTP headers.")
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error checking software version: {e}")

    def check_domain_age(self):
        self.logger.info("Checking domain age...")
        try:
            extract_result = tldextract.extract(self.target_url)
            domain = extract_result.domain + '.' + extract_result.suffix

            domain_info = whois.whois(domain)
            creation_date = domain_info.creation_date
            if isinstance(creation_date, list):
                creation_date = creation_date[0]

            current_date = datetime.datetime.now()
            domain_age = (current_date - creation_date).days
            self.logger.info(f"Domain Age: {domain_age} days")
            self.vulnerabilities.append({'type': 'Domain Age', 'details': f"Domain Age: {domain_age} days"})
        except Exception as e:
            self.logger.error(f"Error retrieving domain information: {e}")

    def scan_directory_traversal(self):
        self.logger.info("Scanning for Directory Traversal vulnerabilities...")
        try:
            response = self.session.get(self.target_url + "../../../../../../../../etc/passwd")

            if response.status_code == 200:
                self.logger.warning("Directory Traversal vulnerability found.")
                self.vulnerabilities.append({'type': 'Directory Traversal', 'details': 'Directory Traversal vulnerability found.'})
            else:
                self.logger.info("No Directory Traversal vulnerability detected.")
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Error scanning for Directory Traversal vulnerabilities: {e}")

    def scan_subdomain_takeover(self):
        self.logger.info("Scanning for Subdomain Takeover vulnerabilities...")
        # Implement subdomain takeover scan logic here
        self.logger.info("Subdomain Takeover scan completed.")

    def run_scan(self):
        self.logger.info(f"Starting vulnerability scan for {self.target_url}")
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_sql = executor.submit(self.scan_sql_injection)
            future_phishing = executor.submit(self.scan_phishing)
            future_xss = executor.submit(self.scan_xss)
            future_outdated = executor.submit(self.scan_outdated_software)
            future_domain = executor.submit(self.check_domain_age)
            future_directory_traversal = executor.submit(self.scan_directory_traversal)
            future_subdomain_takeover = executor.submit(self.scan_subdomain_takeover)

        if self.vulnerabilities:
            self.logger.info("\nVulnerability Report:")
            for idx, vulnerability in enumerate(self.vulnerabilities, start=1):
                self.logger.info(f"{idx}. Type: {vulnerability['type']}\n   Details: {vulnerability['details']}")

        self.logger.info("\nVulnerability scan completed.")


def extract_version_from_header(header):
    match = re.search(r'\b\d+\.\d+\.\d+\b', header)
    if match:
        return match.group()
    else:
        return None

def is_outdated(current_version, threshold_version):
    return version.parse(current_version) < version.parse(threshold_version)


target_url = "http://www.example.com"
scanner = WebVulnerabilityScanner(target_url)
scanner.run_scan()
